@book{bertsekasReinforcementLearningOptimal2019,
  title = {Reinforcement Learning and Optimal Control},
  author = {Bertsekas, Dimitri P},
  year = {2019},
  edition = {1st},
  publisher = {{Athena Scientific}},
  isbn = {978-1-886529-39-7},
  keywords = {PT1,R20_1,RL},
  language = {English}
}

@book{busoniuReinforcementLearningDynamic2017,
  title = {Reinforcement {{Learning}} and {{Dynamic Programming Using Function Approximators}}},
  author = {Busoniu, Lucian and Babuska, Robert and Schutter, Bart De and Ernst, Damien},
  year = {2017},
  month = jul,
  publisher = {{CRC Press}},
  abstract = {From household appliances to applications in robotics, engineered systems involving complex dynamics can only be as effective as the algorithms that control them. While Dynamic Programming (DP) has provided researchers with a way to optimally solve decision and control problems involving complex dynamic systems, its practical value was limited by algorithms that lacked the capacity to scale up to realistic problems.  However, in recent years, dramatic developments in Reinforcement Learning (RL), the model-free counterpart of DP, changed our understanding of what is possible. Those developments led to the creation of reliable methods that can be applied even when a mathematical model of the system is unavailable, allowing researchers to solve challenging control problems in engineering, as well as in a variety of other disciplines, including economics, medicine, and artificial intelligence.  Reinforcement Learning and Dynamic Programming Using Function Approximators provides a comprehensive and unparalleled exploration of the field of RL and DP. With a focus on continuous-variable problems, this seminal text details essential developments that have substantially altered the field over the past decade. In its pages, pioneering experts provide a concise introduction to classical RL and DP, followed by an extensive presentation of the state-of-the-art and novel methods in RL and DP with approximation. Combining algorithm development with theoretical guarantees, they elaborate on their work with illustrative examples and insightful comparisons. Three individual chapters are dedicated to representative algorithms from each of the major classes of techniques: value iteration, policy iteration, and policy search. The features and performance of these algorithms are highlighted in extensive experimental studies on a range of control applications.  The recent development of applications involving complex systems has led to a surge of interest in RL and DP methods and the subsequent need for a quality resource on the subject. For graduate students and others new to the field, this book offers a thorough introduction to both the basics and emerging methods. And for those researchers and practitioners working in the fields of optimal and adaptive control, machine learning, artificial intelligence, and operations research, this resource offers a combination of practical algorithms, theoretical analysis, and comprehensive examples that they will be able to adapt and apply to their own work.  Access the authors' website at www.dcsc.tudelft.nl/rlbook/ for additional material, including computer code used in the studies and information concerning new developments.},
  annotation = {ZSCC: NoCitationData[s2]},
  googlebooks = {xpwuDwAAQBAJ},
  isbn = {978-1-351-83382-0},
  keywords = {Computers / Machine Theory,PT1,RL,Technology \& Engineering / Electrical,Technology \& Engineering / Electronics / General},
  language = {en}
}

@article{campiFiniteSampleProperties2002,
  ids = {campi2002b},
  title = {Finite Sample Properties of System Identification Methods},
  author = {Campi, Marco C. and Weyer, Erik},
  year = {2002},
  volume = {47},
  pages = {1329--1334},
  publisher = {{IEEE}},
  issn = {00189286},
  doi = {10.1109/TAC.2002.800750},
  abstract = {In this paper we study the quality of system identification models obtained using the standard quadratic prediction error criterion for a general linear model class. The main feature of our results is that they hold true for a finite data sample and they are not asymptotic. The main theorems bound the difference between the expected value of the identification criterion evaluated at the estimated parameters and at the optimal parameters. The bound depends naturally on the model and system order, the pole locations, and the noise variance, and it shows that although these variables often do not enter in asymptotic convergence results, they do play an important role when the data sample is finite.},
  annotation = {ZSCC: 0000074},
  file = {/home/joao/DADOS/GDRIVE/Zotero/2002 [campi_weyer] finite sample properties of system identification methods.pdf},
  isbn = {0018-9286 VO - 47},
  journal = {IEEE Transactions on Automatic Control},
  keywords = {DD,Finite sample,Nonasymptotic theory,Prediction error methods,PT1,R20_1,RL,System identification,VRFT},
  number = {8}
}

@article{deanSampleComplexityLinear2017,
  title = {On the {{Sample Complexity}} of the {{Linear Quadratic Regulator}}},
  author = {Dean, Sarah and Mania, Horia and Matni, Nikolai and Recht, Benjamin and Tu, Stephen},
  year = {2017},
  doi = {arXiv:1710.01688v3},
  abstract = {This paper addresses the optimal control problem known as the Linear Quadratic Regulator in the case when the dynamics are unknown. We propose a multi-stage procedure, called Coarse-ID control, that estimates a model from a few experimental trials, estimates the error in that model with respect to the truth, and then designs a controller using both the model and uncertainty estimate. Our technique uses contemporary tools from random matrix theory to bound the error in the estimation procedure. We also employ a recently developed approach to control synthesis called System Level Synthesis that enables robust control design by solving a convex optimization problem. We provide end-to-end bounds on the relative error in control cost that are nearly optimal in the number of parameters and that highlight salient properties of the system to be controlled such as closed-loop sensitivity and optimal control magnitude. We show experimentally that the Coarse-ID approach enables efficient computation of a stabilizing controller in regimes where simple control schemes that do not take the model uncertainty into account fail to stabilize the true system.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1710.01688},
  eprinttype = {arxiv},
  file = {/home/joao/DADOS/GDRIVE/Zotero/2017 [dean et al] on the sample complexity of the linear quadratic regulator.pdf},
  keywords = {PT1,R20_1,RL}
}

@article{lewisReinforcementLearningFeedback2012,
  ids = {2012,lewis2012},
  title = {Reinforcement Learning and Feedback Control: {{Using}} Natural Decision Methods to Design Optimal Adaptive Controllers},
  author = {Lewis, Frank L. and Vrabie, Draguna and Vamvoudakis, Kyriakos G.},
  year = {2012},
  volume = {32},
  pages = {76--105},
  publisher = {{IEEE}},
  issn = {1066033X},
  doi = {10.1109/MCS.2012.2214134},
  abstract = {This article describes the use of principles of reinforcement learning to design feedback controllers for discrete- and continuous-time dynamical systems that combine features of adaptive control and optimal control. Adaptive control [1], [2] and optimal control [3] represent different philosophies for designing feedback controllers. Optimal controllers are normally designed of ine by solving Hamilton JacobiBellman (HJB) equations, for example, the Riccati equation, using complete knowledge of the system dynamics. Determining optimal control policies for nonlinear systems requires the offline solution of nonlinear HJB equations, which are often difficult or impossible to solve. By contrast, adaptive controllers learn online to control unknown systems using data measured in real time along the system trajectories. Adaptive controllers are not usually designed to be optimal in the sense of minimizing user-prescribed performance functions. Indirect adaptive controllers use system identification techniques to first identify the system parameters and then use the obtained model to solve optimal design equations [1]. Adaptive controllers may satisfy certain inverse optimality conditions [4].},
  annotation = {ZSCC: 0000511},
  file = {/home/joao/DADOS/GDRIVE/Zotero/2012 [lewis et al] reinforcement learning and feedback control.pdf},
  journal = {IEEE Control Systems},
  keywords = {actor-critic,DD,PT1,R20_1,RL},
  number = {6}
}

@inproceedings{liuAttitudeSynchronizationMultiple2019,
  title = {Attitude {{Synchronization}} for {{Multiple Quadrotors}} Using {{Reinforcement Learning}}*},
  booktitle = {2019 {{Chinese Control Conference}} ({{CCC}})},
  author = {Liu, Hao and Zhao, Wanbing and Lewis, Frank L. and Jiang, Zhong-Ping and Modares, Hamidreza},
  year = {2019},
  month = jul,
  pages = {2480--2483},
  issn = {1934-1768},
  doi = {10/gg5nkr},
  abstract = {In this paper, a reinforcement learning based control law is proposed to solve the attitude synchronization problem of the leader-following multi-quadrotor systems. The overall system is composed of a team of quadrotors, modeled with highly nonlinear and coupled dynamics. An optimal control solution is obtained by solving an augmented Hamilton-Jacobi-Bellman equation. A reinforcement learning approach is used to learn the optimal control law. Simulation results are provided to verify the effectiveness of the proposed controller.},
  annotation = {ZSCC: 0000000},
  file = {/home/joao/DADOS/GDRIVE/Zotero/2019 [liu et al] attitude synchronization for multiple quadrotors using reinforcement learning2.pdf;/home/joao/Zotero/storage/JSEPW3VM/8865177.html},
  keywords = {aircraft control,attitude control,Attitude control,attitude synchronization problem,augmented Hamilton-Jacobi-Bellman equation,control engineering computing,control system synthesis,helicopters,highly nonlinear coupled dynamics,leader-following multiquadrotor systems,learning (artificial intelligence),Mathematical model,multiple quadrotors,nonlinear control systems,Nonlinear dynamical systems,Observers,optimal control,Optimal control,optimal control law,optimal control solution,PT1,Reinforcement learning,reinforcement learning approach,reinforcement learning based control law,RL,stochastic processes,synchronisation,Synchronization}
}

@article{rechtTourReinforcementLearning2018,
  ids = {recht2019},
  title = {A {{Tour}} of {{Reinforcement Learning}}: {{The View}} from {{Continuous Control}}},
  author = {Recht, Benjamin},
  year = {2018},
  pages = {1--28},
  issn = {1471-3012},
  doi = {arXiv:1806.09460v2},
  abstract = {This manuscript surveys reinforcement learning from the perspective of optimization and control with a focus on continuous control applications. It surveys the general formulation, terminology, and typical experimental implementations of reinforcement learning and reviews competing solution paradigms. In order to compare the relative merits of various techniques, this survey presents a case study of the Linear Quadratic Regulator (LQR) with unknown dynamics, perhaps the simplest and best-studied problem in optimal control. The manuscript describes how merging techniques from learning theory and control can provide non-asymptotic characterizations of LQR performance and shows that these characterizations tend to match experimental behavior. In turn, when revisiting more complex applications, many of the observed phenomena in LQR persist. In particular, theory and experiment demonstrate the role and importance of models and the cost of generality in reinforcement learning algorithms. This survey concludes with a discussion of some of the challenges in designing learning systems that safely and reliably interact with complex and uncertain environments and how tools from reinforcement learning and control might be combined to approach these challenges.},
  annotation = {ZSCC: 0000002},
  archivePrefix = {arXiv},
  eprint = {1806.09460},
  eprinttype = {arxiv},
  file = {/home/joao/DADOS/GDRIVE/Zotero/2018 [recht] a tour of reinforcement learning.pdf;/home/joao/DADOS/GDRIVE/Zotero/2018 [recht] a tour of reinforcement learning2.pdf},
  isbn = {1471-3012},
  keywords = {PT1,R20_1,RL}
}

@book{saitoPythonReinforcementLearning2018,
  title = {Python Reinforcement Learning Projects Eight Hands-on Projects Exploring Reinforcement Learning Algorithms Using {{TensorFlow}}},
  author = {Saito, Sean and Wenzhuo, Yang and Shanmugamani, Rajalingappaa},
  year = {2018},
  abstract = {Python Reinforcement Learning Projects brings various aspects and methodologies of RL using 8 real-world projects that explore RL and will have hands-on experience with real data and artificial intelligence problems. You will learn to build self-learning models using sophisticated techniques like Q-learning, Markov models and Monte-Carlo process.},
  annotation = {ZSCC: 0000004  OCLC: 1144179038},
  isbn = {978-1-78899-161-2 978-1-78899-322-7},
  keywords = {PT1,RL},
  language = {English}
}

@article{sathiyakeerthiTutorialSurveyReinforcement1994,
  title = {A Tutorial Survey of Reinforcement Learning},
  author = {Sathiya Keerthi, S and Ravindran, B},
  year = {1994},
  month = dec,
  volume = {19},
  pages = {851--889},
  issn = {0256-2499, 0973-7677},
  doi = {10/c83b9x},
  annotation = {ZSCC: 0000070},
  file = {/home/joao/DADOS/GDRIVE/Zotero/1994 [sathiya keerthi_ravindran] a tutorial survey of reinforcement learning.pdf},
  journal = {Sadhana},
  keywords = {PT1,R20_1,RL},
  language = {en},
  number = {6}
}

@article{simchowitzLearningMixingSharp2018,
  title = {Learning {{Without Mixing}}: {{Towards A Sharp Analysis}} of {{Linear System Identification}}},
  author = {Simchowitz, Max and Mania, Horia and Tu, Stephen and Jordan, Michael I and Recht, Benjamin},
  year = {2018},
  volume = {75},
  pages = {1--35},
  abstract = {We prove that the ordinary least-squares (OLS) estimator attains nearly minimax optimal performance for the identification of linear dynamical systems from a single observed trajectory. Our upper bound relies on a generalization of Mendelson's small-ball method to dependent data, eschewing the use of standard mixing-time arguments. Our lower bounds reveal that these upper bounds match up to logarithmic factors. In particular, we capture the correct signal-to-noise behavior of the problem, showing that more unstable linear systems are easier to estimate. This behavior is qualitatively different from arguments which rely on mixing-time calculations that suggest that unstable systems are more difficult to estimate. We generalize our technique to provide bounds for a more general class of linear response time-series.},
  annotation = {ZSCC: 0000092},
  archivePrefix = {arXiv},
  eprint = {1802.08334},
  eprinttype = {arxiv},
  file = {/home/joao/DADOS/GDRIVE/Zotero/2018 [simchowitz et al] learning without mixing.pdf},
  journal = {Proceedings of Machine Learning Research},
  keywords = {autoregressive processes,empirical process theory,Linear dynamical systems,PT1,R20_1,RL,system identification,time series}
}

@article{siOnlineLearningControl2001,
  title = {Online Learning Control by Association and Reinforcement},
  author = {Si, J. and {Yu-Tsung Wang}},
  year = {2001},
  month = mar,
  volume = {12},
  pages = {264--276},
  issn = {10459227},
  doi = {10/c7tvgh},
  annotation = {ZSCC: 0000704},
  journal = {IEEE Transactions on Neural Networks},
  keywords = {PT1,R20_1,RL},
  number = {2}
}

@book{sutton2018,
  ids = {2018},
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  edition = {Second edition},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts}},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  isbn = {978-0-262-03924-6},
  keywords = {\#nosource,PT1,R20_1,Reinforcement learning,RL},
  lccn = {Q325.6 .R45 2018},
  series = {Adaptive Computation and Machine Learning Series}
}

@article{umenbergerLearningConvexBounds2018,
  title = {Learning Convex Bounds for Linear Quadratic Control Policy Synthesis},
  author = {Umenberger, Jack and Sch{\"o}n, Thomas B.},
  year = {2018},
  month = jun,
  abstract = {Learning to make decisions from observed data in dynamic environments remains a problem of fundamental importance in a number of fields, from artificial intelligence and robotics, to medicine and finance. This paper concerns the problem of learning control policies for unknown linear dynamical systems so as to maximize a quadratic reward function. We present a method to optimize the expected value of the reward over the posterior distribution of the unknown system parameters, given data. The algorithm involves sequential convex programing, and enjoys reliable local convergence and robust stability guarantees. Numerical simulations and stabilization of a real-world inverted pendulum are used to demonstrate the approach, with strong performance and robustness properties observed in both.},
  annotation = {ZSCC: 0000005},
  archivePrefix = {arXiv},
  eprint = {1806.00319},
  eprinttype = {arxiv},
  file = {/home/joao/DADOS/GDRIVE/Zotero/2018 [umenberger_schon] learning convex bounds for linear quadratic control policy synthesis.pdf},
  journal = {arXiv:1806.00319 [cs, math, stat]},
  keywords = {Computer Science - Machine Learning,DD,Mathematics - Optimization and Control,PT1,R20_1,RL,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, math, stat}
}

@article{umenbergerLearningConvexBounds2018a,
  title = {Learning Convex Bounds for Linear Quadratic Control Policy Synthesis},
  author = {Umenberger, Jack and Sch{\"o}n, Thomas B.},
  year = {2018},
  month = jun,
  abstract = {Learning to make decisions from observed data in dynamic environments remains a problem of fundamental importance in a number of fields, from artificial intelligence and robotics, to medicine and finance. This paper concerns the problem of learning control policies for unknown linear dynamical systems so as to maximize a quadratic reward function. We present a method to optimize the expected value of the reward over the posterior distribution of the unknown system parameters, given data. The algorithm involves sequential convex programing, and enjoys reliable local convergence and robust stability guarantees. Numerical simulations and stabilization of a real-world inverted pendulum are used to demonstrate the approach, with strong performance and robustness properties observed in both.},
  annotation = {ZSCC: 0000005},
  archivePrefix = {arXiv},
  eprint = {1806.00319},
  eprinttype = {arxiv},
  file = {/home/joao/DADOS/GDRIVE/Zotero/2018 [umenberger_schon] learning convex bounds for linear quadratic control policy synthesis2.pdf;/home/joao/Zotero/storage/2TMJX687/1806.html},
  journal = {arXiv:1806.00319 [cs, math, stat]},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Mathematics - Optimization and Control,PT1,RL,Statistics - Machine Learning},
  primaryClass = {cs, math, stat}
}

@article{umenbergerRobustExplorationLinear2019,
  ids = {umenberger2019a},
  title = {Robust Exploration in Linear Quadratic Reinforcement Learning},
  author = {Umenberger, Jack and Ferizbegovic, Mina and Sch{\"o}n, Thomas B. and Hjalmarsson, H{\aa}kan},
  year = {2019},
  month = jun,
  abstract = {This paper concerns the problem of learning control policies for an unknown linear dynamical system to minimize a quadratic cost function. We present a method, based on convex optimization, that accomplishes this task robustly: i.e., we minimize the worst-case cost, accounting for system uncertainty given the observed data. The method balances exploitation and exploration, exciting the system in such a way so as to reduce uncertainty in the model parameters to which the worst-case cost is most sensitive. Numerical simulations and application to a hardware-in-the-loop servo-mechanism demonstrate the approach, with appreciable performance and robustness gains over alternative methods observed in both.},
  annotation = {ZSCC: 0000008},
  archivePrefix = {arXiv},
  eprint = {1906.01584},
  eprinttype = {arxiv},
  file = {/home/joao/DADOS/GDRIVE/Zotero/2019 [umenberger et al] robust exploration in linear quadratic reinforcement learning.pdf},
  journal = {arXiv:1906.01584 [cs, math, stat]},
  keywords = {Computer Science - Machine Learning,DD,Folder - RL,Mathematics - Optimization and Control,PT1,RL,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, math, stat}
}

@inproceedings{vamvoudakisOnlineAdaptiveLearning2011,
  title = {Online Adaptive Learning of Optimal Control Solutions Using Integral Reinforcement Learning},
  booktitle = {2011 {{IEEE Symposium}} on {{Adaptive Dynamic Programming}} and {{Reinforcement Learning}} ({{ADPRL}})},
  author = {Vamvoudakis, Kyriakos G. and Vrabie, Draguna and Lewis, Frank L.},
  year = {2011},
  month = apr,
  pages = {250--257},
  issn = {2325-1867},
  doi = {10/fj84tj},
  abstract = {In this paper we introduce an online algorithm that uses integral reinforcement knowledge for learning the continuous-time optimal control solution for nonlinear systems with infinite horizon costs and partial knowledge of the system dynamics. This algorithm is a data based approach to the solution of the Hamilton-Jacobi-Bellman equation and it does not require explicit knowledge on the system's drift dynamics. The adaptive algorithm is based on policy iteration, and it is implemented on an actor/critic structure. Both actor and critic neural networks are adapted simultaneously a persistence of excitation condition is required to guarantee convergence of the critic to the actual optimal value function. Novel tuning algorithms are given for both critic and actor networks, with extra terms in the actor tuning law being required to guarantee closed-loop dynamical stability. The convergence to the optimal controller is proven, and stability of the system is also guaranteed. Simulation examples support the theoretical result.},
  annotation = {ZSCC: 0000012},
  file = {/home/joao/DADOS/GDRIVE/Zotero/2011 [vamvoudakis et al] online adaptive learning of optimal control solutions using integral.pdf;/home/joao/Zotero/storage/JMJ3N5RW/5967359.html},
  keywords = {actor neural networks,adaptive control,closed loop systems,closed-loop dynamical stability,continuous time systems,continuous-time optimal control solution,critic neural networks,Hamilton-Jacobi-Bellman equation,infinite horizon costs,integral reinforcement knowledge,integral reinforcement learning,iterative methods,Jacobian matrices,learning (artificial intelligence),learning systems,neurocontrollers,nonlinear control systems,nonlinear systems,online adaptive learning,optimal control,policy iteration,PT1,RL,stability,system dynamics,tuning algorithms}
}

@book{vrabieOptimalAdaptiveControl2013,
  title = {Optimal Adaptive Control and Differential Games by Reinforcement Learning Principles},
  author = {Vrabie, Draguna L.},
  year = {2013},
  publisher = {{The Institution of Engineering and Technology}},
  address = {{London}},
  annotation = {ZSCC: NoCitationData[s1]},
  collaborator = {{Institution of Engineering {and} Technology}},
  isbn = {978-1-84919-489-1},
  keywords = {Adaptive control systems,PT1,RL},
  lccn = {MLCM 2017/45857 (T)},
  number = {81},
  series = {{{IET}} Control Engineering Series}
}

@article{watkinsLearningDelayedRewards1989,
  title = {Learning with {{Delayed Rewards}}},
  author = {Watkins, Christopher J. C. H.},
  year = {1989},
  volume = {15},
  pages = {233--235},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/joao/DADOS/GDRIVE/Zotero/1989 [watkins] learning with delayed rewards.pdf},
  journal = {Unpublished Ph.d.thesis},
  keywords = {PT1,R20_1,RL},
  number = {4}
}

@article{watkinsQlearning1992,
  ids = {watkins1992},
  title = {Q-Learning},
  author = {Watkins, Christopher J. C. H. and Dayan, Peter},
  year = {1992},
  volume = {8},
  pages = {279--292},
  issn = {0885-6125},
  doi = {10.1007/bf00992698},
  abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem forQ-learning based on that outlined in Watkins (1989). We show thatQ-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where manyQ values can be changed each iteration, rather than just one.},
  annotation = {ZSCC: 0010899},
  file = {/home/joao/DADOS/GDRIVE/Zotero/1992 [watkins_dayan] q-learning.pdf},
  journal = {Machine Learning},
  keywords = {-learning,asynchronous dynamic programming,PT1,R20_1,reinforcement learning,RL,temporal differences},
  number = {3-4}
}

@article{wenOptimizedBacksteppingTracking2018,
  ids = {OptimizedBacksteppingTracking2018,Wen2018,wen2018a,wen2018b,wen2018c,wen2018d},
  title = {Optimized {{Backstepping}} for {{Tracking Control}} of {{Strict}}-{{Feedback Systems}}},
  author = {Wen, Guoxing and Ge, Shuzhi Sam and Tu, Fangwen},
  year = {2018},
  month = aug,
  volume = {29},
  pages = {3850--3862},
  publisher = {{IEEE}},
  issn = {2162-237X},
  doi = {10.1109/TNNLS.2018.2803726},
  abstract = {In this paper, a control technique named optimized backstepping is first proposed by implementing tracking control for a class of strict-feedback systems, which considers optimization as a design philosophy of the high-order system control. The basic idea is that designing the actual and virtual controls of backstepping is the optimized solutions of the corresponding subsystems so that overall control of the high-order system is optimized. In general, optimization control is designed based on the solution of Hamilton-Jacobi-Bellman equation, but solving the equation is very difficult due to the inherent nonlinearity and intractability. In order to overcome the difficulty, the neural network (NN)-based reinforcement learning strategy of actor-critic architecture is used. In every backstepping step, the actor and critic NNs are constructed for executing control behavior and evaluating control performance, respectively. According to the Lyapunov stability theorem, it is proven that the desired control performance can be obtained. Finally, a simulation example is carried out to further demonstrate the effectiveness of the proposed control approach.},
  annotation = {ZSCC: 0000031},
  file = {/home/joao/DADOS/GDRIVE/Zotero/2018 [wen et al] optimized backstepping for tracking control of strict-feedback systems.pdf},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  keywords = {actor-critic,Backstepping,Lyapunov stability,optimized backstepping (OB),PT1,R20_1,RL,strict-feedback system,TCNL,tracking control},
  number = {8}
}


